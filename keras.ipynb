{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=pd.read_csv(\"Training Data 2017 2018.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=dataframe.drop(\"Unnamed: 0\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "dataframe_s = shuffle(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=dataframe_s.iloc[:2401,:]\n",
    "test=dataframe_s.iloc[2402:2835,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.dropna()\n",
    "train=train.drop(columns=['Date'])\n",
    "train['label']=train['label']+2\n",
    "test=test.dropna()\n",
    "test['label']=test['label']+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qinke\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_class= (2398, 5)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_train=to_categorical(train['label'])\n",
    "num_class=y_train.shape[1]\n",
    "print(\"num_class=\",y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=tokenizer.texts_to_matrix(train['text'])\n",
    "x_test=tokenizer.texts_to_matrix(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2398, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2398, 5000, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=x_train.reshape(x_train.shape[0],5000,1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train,x_val,y_train,y_val=train_test_split(x_train,train['label'],test_size=0.10,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(433, 5000, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test=x_test.reshape(x_test.shape[0],5000,1)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import BatchNormalization,Convolution2D, MaxPooling2D\n",
    "from keras.layers import Dense , Dropout, Lambda, Flatten\n",
    "from keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model=Sequential()\n",
    "model.add(Dense(50,input_shape=(5000, 1)))\n",
    "#here the standardize is the function we define upwords\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5,activation='softmax'))\n",
    "#only with softmax, it is a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD \n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2398, 5000, 1)\n",
      "(2398, 5)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
      "Epoch 1/20\n",
      "2398/2398 [==============================] - 13s 6ms/step - loss: 1.5194 - acc: 0.3440\n",
      "Epoch 2/20\n",
      "2398/2398 [==============================] - 14s 6ms/step - loss: 1.4682 - acc: 0.3670\n",
      "Epoch 3/20\n",
      "2398/2398 [==============================] - 13s 6ms/step - loss: 1.4479 - acc: 0.3837\n",
      "Epoch 4/20\n",
      "2398/2398 [==============================] - 14s 6ms/step - loss: 1.4204 - acc: 0.3891\n",
      "Epoch 5/20\n",
      "2398/2398 [==============================] - 14s 6ms/step - loss: 1.3625 - acc: 0.4237\n",
      "Epoch 6/20\n",
      "2398/2398 [==============================] - 13s 5ms/step - loss: 1.2906 - acc: 0.4775\n",
      "Epoch 7/20\n",
      "2398/2398 [==============================] - 13s 5ms/step - loss: 1.1848 - acc: 0.5567\n",
      "Epoch 8/20\n",
      "2398/2398 [==============================] - 14s 6ms/step - loss: 1.0424 - acc: 0.6230\n",
      "Epoch 9/20\n",
      "2398/2398 [==============================] - 14s 6ms/step - loss: 0.8820 - acc: 0.7048\n",
      "Epoch 10/20\n",
      "2398/2398 [==============================] - 15s 6ms/step - loss: 0.7194 - acc: 0.7823\n",
      "Epoch 11/20\n",
      "2398/2398 [==============================] - 14s 6ms/step - loss: 0.5722 - acc: 0.8344\n",
      "Epoch 12/20\n",
      "2398/2398 [==============================] - 15s 6ms/step - loss: 0.4456 - acc: 0.8841\n",
      "Epoch 13/20\n",
      "2398/2398 [==============================] - 16s 7ms/step - loss: 0.3427 - acc: 0.9208\n",
      "Epoch 14/20\n",
      "2398/2398 [==============================] - 14s 6ms/step - loss: 0.2665 - acc: 0.9404\n",
      "Epoch 15/20\n",
      "2398/2398 [==============================] - 13s 6ms/step - loss: 0.2082 - acc: 0.9566\n",
      "Epoch 16/20\n",
      "2398/2398 [==============================] - 13s 6ms/step - loss: 0.1685 - acc: 0.9662\n",
      "Epoch 17/20\n",
      "2398/2398 [==============================] - 14s 6ms/step - loss: 0.1375 - acc: 0.9746\n",
      "Epoch 18/20\n",
      "2398/2398 [==============================] - 15s 6ms/step - loss: 0.1170 - acc: 0.9758\n",
      "Epoch 19/20\n",
      "2398/2398 [==============================] - 14s 6ms/step - loss: 0.0993 - acc: 0.9808\n",
      "Epoch 20/20\n",
      "2398/2398 [==============================] - 13s 6ms/step - loss: 0.0874 - acc: 0.9821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x189d1d126d8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,train['label'], epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.3077194e-03, 4.6481395e-01, 4.3460868e-02, 2.4855451e-01,\n",
       "        2.3886295e-01],\n",
       "       [4.4322088e-03, 7.8348684e-01, 7.6872513e-02, 1.7269235e-02,\n",
       "        1.1793912e-01],\n",
       "       [4.1038085e-02, 2.6107657e-01, 3.3931970e-03, 6.7635900e-01,\n",
       "        1.8133132e-02],\n",
       "       ...,\n",
       "       [2.5211718e-02, 9.5527536e-01, 1.0456561e-05, 1.1803496e-03,\n",
       "        1.8322160e-02],\n",
       "       [2.0593813e-01, 1.5398714e-01, 6.4981826e-02, 3.6100093e-01,\n",
       "        2.1409205e-01],\n",
       "       [5.0693364e-03, 8.7223998e-05, 1.4117015e-02, 9.6574056e-01,\n",
       "        1.4985809e-02]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model,to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(50,input_shape=(5000, 1)))\n",
    "#here the standardize is the function we define upwords\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5,activation='softmax'))\n",
    "#only with softmax, it is a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 \n",
    "nb_classes = 5\n",
    "nb_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = LossHistory()\n",
    "model.fit(X_train, Y_train,\n",
    "            batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "            verbose=1, \n",
    "            validation_data=(X_test, Y_test),\n",
    "            callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.loss_plot('epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
